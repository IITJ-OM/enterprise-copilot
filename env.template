# ============================================
# Agentic Cache-Driven Application Config
# ============================================
# Copy this file to .env and fill in your values

# ==================== API KEYS ====================
# At least one LLM API key is required

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here

# Google Gemini Configuration
GOOGLE_API_KEY=your_google_api_key_here


# ==================== REDIS (Layer 0) ====================
# Redis configuration for exact cache

REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=


# ==================== QDRANT (Layer 1 & 2) ====================
# Qdrant vector database configuration

QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_API_KEY=

# Connection settings
# Set QDRANT_HTTPS=true for Qdrant Cloud or production instances with SSL
# For local Docker instances, keep as false
QDRANT_HTTPS=false

# Set QDRANT_PREFER_GRPC=true to use gRPC protocol (port 6334)
# For HTTP REST API, keep as false (port 6333)
QDRANT_PREFER_GRPC=false


# ==================== LLM CONFIGURATION ====================
# Default LLM provider and models

DEFAULT_LLM=openai
OPENAI_MODEL=gpt-3.5-turbo
GEMINI_MODEL=gemini-pro


# ==================== CACHE SETTINGS ====================
# Cache behavior configuration

# Similarity threshold for semantic cache (0.0 to 1.0)
# Higher = more strict matching
SEMANTIC_SIMILARITY_THRESHOLD=0.85

# Similarity threshold for RAG cache (0.0 to 1.0)
RAG_SIMILARITY_THRESHOLD=0.75

# Time-to-live for Layer 0 cache in seconds
CACHE_TTL=3600


# ==================== CHUNKING SETTINGS ====================
# Text chunking configuration for RAG (Layer 2)

# Enable/disable automatic text chunking for documents
ENABLE_CHUNKING=true

# Chunk size in tokens
# Smaller = more precise retrieval, Larger = more context per chunk
# Recommended: 256-1024 tokens
CHUNK_SIZE=512

# Overlap between chunks in tokens
# Helps maintain context across chunk boundaries
# Recommended: 10-20% of chunk size
CHUNK_OVERLAP=50

# Chunking strategy
# Options: "recursive" (split on natural boundaries), "fixed" (fixed-size chunks)
# Recommended: "recursive" for better semantic coherence
CHUNKING_STRATEGY=recursive


# ==================== EMBEDDING MODEL ====================
# Sentence transformer model for embeddings
# Options: all-MiniLM-L6-v2 (fast), all-mpnet-base-v2 (accurate)

EMBEDDING_MODEL=all-MiniLM-L6-v2


# ==================== DEBUG SETTINGS ====================
# Enable debug logging for detailed operation insights

# Set to true to enable verbose debug logging
# Helps with troubleshooting and understanding cache behavior
DEBUG=false

